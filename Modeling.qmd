---
title: "Modeling"
format: html
editor: visual
---

# Introduction

Diabetes is among the most prevalent chronic diseases in the United States, impacting millions of American each year and exerting a significant financial burden on the economy. Diabetes is a serious chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy.

Complications like heart disease, vision loss, lower-limb amputation, and kidney disease are associated with chronically high levels of sugar remaining in the bloodstream for those with diabetes. While there is no cure, strategies like losing weight, eating healthy, being active, and receiving medical treatments can mitigate the harms of this disease in many patients. Early diagnosis can lead to lifestyle changes and more effective treatment, making predictive models for diabetes risk important tools for public and public health officials.

The data set used `diabetes_binary_health_indicators_BRFSS2015.csv` is a clean dataset of 253,680 survey responses to the CDC's Behavioral Risk Factor Surveillance System (BRFSS) 2015 survey. The target variable `Diabetes_binary` has 2 classes: 0 for no diabetes and 1 for prediabetes or diabetes. While the data set has 21 feature variables, for the purposes of this analysis, we will use five predictor variables.

  - `HighBP`: Binary variable where 0 is no high blood pressure (BP) and 1 is high BP
  - `HighChol`: Binary variable where 0 is no high cholesterol and 1 is high cholesterol
  - `BMI`: Continuous variable for Body Mass Index
  - `HeartDiseaseorAttack`: Binary variable for coronary heart disease (CHD) or myocardial infection (MI), where 0 is no and 1 is yes.
  - `PhysActivity`: Binary variable where 0 is no physical activity in the past 30 days (not including job) and 1 is yes (some physical activity)

In our prior page we explored our data to gain insight into the data's structure, variable distribution, and potential relationships between the predictor variables and the `Diabetes_binary` variable. We saw that those who had diabetes often had high blood pressure and high cholesterol. We also saw that there wasn't quite the same relationship between those who had diabetes and those who had heart diseases or attacks and those who were not physically active. Although we will still keep these variables in as predictors.

Now our goal will be to create models for predicting the `Diabetes_binary` variable (using `tidymodels`). We'll use **log-loss** as our metric to evaluate the models. We'll create a classification tree model and a random forest model, using log-loss with 5 fold cross-validation to select the best model from these two families of models.

# Data

We can first start by reading in the data and doing the same data cleaning as in our EDA file (i.e. making the binary variables into factors). We'll also split the data into a training (70% of the data) and a test set (30% of the data). We also can set up our our 5 fold CV split based on the training data.

```{r, warning = FALSE}
library(tidyverse)
library(tidymodels)

full_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

head(full_data)

full_data <- full_data |>
  mutate(
    HighBP = factor(HighBP, levels = c(0,1), labels = c("No", "Yes")),
    HighChol = factor(HighChol, levels = c(0,1), labels = c("No", "Yes")),
    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0,1), labels = c("No", "Yes")),
    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = c("No", "Yes")),
    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = c("No", "Yes"))
  ) |>
  select(Diabetes_binary, HighBP, HighChol, HeartDiseaseorAttack, PhysActivity, BMI)

head(full_data)

set.seed(123)
data_split <- initial_split(full_data, prop = 0.7)
data_train <- training(data_split)
data_test <- testing(data_split)
data_5_fold <- vfold_cv(data_train, 5)

head(data_train)
head(data_test)
```

# Classification Tree

For our first model we will fit a Classification Tree model. Tree based methods are flexible and attempt to split up predictor space into regions. On each region, a different prediction can then be made and adjacent regions do not need to have predictions close to each other. For a Classification Tree model our response in a categorical variable, in our case the `Diabetes_binary` variable, that we try to classify (predict) group membership. We then make our prediction based on which bin an observation ends up in, with the most prevalent class in a bin being used as our prediction. We'll fit a classification tree with varying values for the complexity parameter. We can then use log-loss as the metric based on the 5 fold CV on the training set to evaluate the performance of the different classification tree models to determine the best model in this family.

We can now fit our Classification trees. First we define the recipe, where we can make our categorical variables into dummy values.

```{r}
tree_rec <- recipe(Diabetes_binary ~ ., data = data_train) |>
  step_dummy(HighBP, HighChol, HeartDiseaseorAttack, PhysActivity)
tree_rec
```

Now we define the model, where we can use various values for the tree_depth and cost_complexity, and use a minimal node size of 20 (`min_n` = 20).

```{r}
tree_model <- decision_tree(tree_depth = tune(),
                            min_n = 20,
                            cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

We can then create a workflow object.

```{r}
tree_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_model)
```

We'll use 5 fold CV to select our tuning parameters using a tuning grid. Here we'll set the number of the values ourselves with `grid_regular()`.

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(10,5))
```

Now we can use `tune_grid()` with this specification on our `data_5_fold` object

```{r}
tree_fits <- tree_wkf |>
  tune_grid(resamples = data_5_fold,
            grid = tree_grid,
            metrics = metric_set(mn_log_loss))
tree_fits
```

We can then see which tuning parameters are best.

```{r}
tree_fits |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

We can then grab our best classification tree model, which has a `cost_complexity` of 1e-10 and a `tree_depth` of 11.

```{r}
best_tree <- select_best(tree_fits, metric = "mn_log_loss")
best_tree
```

We can then refit on the entire training set using these tuning parameters.

```{r}
tree_final_wkf <- tree_wkf |>
  finalize_workflow(best_tree)
tree_final_fit <- tree_final_wkf |>
  last_fit(data_split, metrics = metric_set(mn_log_loss))
```

Now we can see how it performed on the test set metrics. We'll want to compare this to the best model from the Random Forest family (which we we'll fit in the next section) to determine which is the best model out of these two families of models.

```{r}
tree_final_fit |> collect_metrics()
```

# Random Forest

Now we will move onto our second family of models, a Random Forest model. A random forest model is an ensemble method that combines the predictions of multiple decision trees to make a more robust and accurate prediction. Instead of relying on one tree, it generates many fitted trees and aggregates their predictions. For classification trees, it usually combines the predictions from all of the fitted classification trees and averages the results using the majority vote, or the most common prediction made by all of the bootstrap trees. What makes a random forest model special is that it doesn't use all of the predictors in each step, and instead considers splits using a random subset of predictors each time, where this number `mtry` is a tuning parameter. By randomly selecting a subset of predictors, a good predictor or two won't dominate the tree fits.

We can now define our random forest model, with the same recipe as before.

```{r}
rf_rec <- recipe(Diabetes_binary ~ ., data = data_train) |>
  step_dummy(HighBP, HighChol, HeartDiseaseorAttack, PhysActivity)
rf_rec
```

We can then define the random forest model.

```{r}
rf_model <- rand_forest(mtry = tune()) |>
  set_engine("ranger") |>
  set_mode("classification")
```

We then combine our recipe and our model into a workflow object.

```{r}
rf_wkf <- workflow() |>
  add_recipe(rf_rec) |>
  add_model(rf_model)
```

We can then fit the random forest model to our 5 CV folds.

```{r}
rf_fit <- rf_wkf |>
  tune_grid(resamples = data_5_fold,
            grid = 7,
            metrics = metric_set(mn_log_loss))
```

We can now examine our log-loss metric across the folds to determine the best `mtry` value.

```{r}
rf_fit |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

We can then grab our best tuning parameter.

```{r}
rf_best_params <- select_best(rf_fit, metric = "mn_log_loss")
rf_best_params
```

We can then refit on the entire training set using this tuning parameter.

```{r}
rf_final_wkf <- rf_wkf |>
  finalize_workflow(rf_best_params)
rf_final_fit <- rf_final_wkf |>
  last_fit(data_split, metrics = metric_set(mn_log_loss))
```

Now we can see how it performs on the test set metrics. In the next section we'll directly compare the two models we fit on their test set metrics to determine the final model and overall winner!

```{r}
rf_final_fit |> collect_metrics()
```

# Final Model Selection

Now we have two *best* models for each model type. For our Classification Tree Model, we determined that the best parameters are a `cost_complexity` of 1e-10 and a `tree_depth` of 11, with our pre-set `min_n` being 20. For our Random Forest Model, the best model had a `mtry` of 3.

We can then compare these models to the test set log-loss metric.

```{r}
tree_final_fit |> collect_metrics()
```

```{r}
rf_final_fit |> collect_metrics()
```

Here we see that based on how our models perform on the test set, our best model and overall winner is the Random Forest model with `mtry` of 3. We'll use this model in our API development.