---
title: "Modeling"
format: html
editor: visual
---

# Introduction

Diabetes is among the most prevalent chronic diseases in the United States, impacting millions of American each year and exerting a significant financial burden on the economy. Diabetes is a serious chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy.

Complications like heart disease, vision loss, lower-limb amputation, and kidney disease are associated with chronically high levels of sugar remaining in the bloodstream for those with diabetes. While there is no cure, strategies like losing weight, eating healthy, being active, and receiving medical treatments can mitigate the harms of this disease in many patients. Early diagnosis can lead to lifestyle changes and more effective treatment, making predictive models for diabetes risk important tools for public and public health officials.

The data set used `diabetes_binary_health_indicators_BRFSS2015.csv` is a clean dataset of 253,680 survey responses to the CDC's Behavioral Risk Factor Surveillance System (BRFSS) 2015 survey. The target variable `Diabetes_binary` has 2 classes: 0 for no diabetes and 1 for prediabetes or diabetes. While the data set has 21 feature variables, for the purposes of this analysis, we will use five predictor variables.

  - `HighBP`: Binary variable where 0 is no high blood pressure (BP) and 1 is high BP
  - `HighChol`: Binary variable where 0 is no high cholesterol and 1 is high cholesterol
  - `BMI`: Continuous variable for Body Mass Index
  - `HeartDiseaseorAttack`: Binary variable for coronary heart disease (CHD) or myocardial infection (MI), where 0 is no and 1 is yes.
  - `PhysActivity`: Binary variable where 0 is no physical activity in the past 30 days (not including job) and 1 is yes (some physical activity)

In our prior page we explored our data to gain insight into the data's structure, variable distribution, and potential relationships between the predictor variables and the `Diabetes_binary` variable. We saw that those who had diabetes often had high blood pressure and high cholesterol. We also saw that there wasn't quite the same relationship between those who had diabetes and those who had heart diseases or attacks and those who were not physically active. Although we will still keep these variables in as predictors.

Now our goal will be to create models for predicting the `Diabetes_binary` variable (using `tidymodels`). We'll use **log-loss** as our metric to evaluate the models. We'll create a classification tree model and a random forest model, using log-loss with 5 fold cross-validation to select the best model from these two families of models.

# Data

We can first start by reading in the data and doing the same data cleaning as in our EDA file (i.e. making the binary variables into factors). We'll also split the data into a training (70% of the data) and a test set (30% of the data). We also can set up our our 5 fold CV split based on the training data.

```{r, warning = FALSE}
library(tidyverse)
library(tidymodels)

full_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

head(full_data)

full_data <- full_data |>
  mutate(
    HighBP = factor(HighBP, levels = c(0,1), labels = c("No", "Yes")),
    HighChol = factor(HighChol, levels = c(0,1), labels = c("No", "Yes")),
    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0,1), labels = c("No", "Yes")),
    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = c("No", "Yes")),
    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = c("No", "Yes"))
  ) |>
  select(Diabetes_binary, HighBP, HighChol, HeartDiseaseorAttack, PhysActivity, BMI)

head(full_data)

set.seed(123)
data_split <- initial_split(full_data, prop = 0.7)
data_train <- training(data_split)
data_test <- testing(data_split)
data_5_fold <- vfold_cv(data_train, 5)

head(data_train)
head(data_test)
```

# Classification Tree

For our first model we will fit a Classification Tree model. Tree based methods are flexible and attempt to split up predictor space into regions. On each region, a different prediction can then be made and adjacent regions do not need to have predictions close to each other. For a Classification Tree model our response in a categorical variable, in our case the `Diabetes_binary` variable, that we try to classify (predict) group membership. We then make our prediction based on which bin an observation ends up in, with the most prevalent class in a bin being used as our prediction. We'll fit a classification tree with varying values for the complexity parameter. We can then use log-loss as the metric based on the 5 fold CV on the training set to evaluate the performance of the different classification tree models to determine the best model in this family.

We can now fit our Classification trees. First we define the recipe, where we can make our categorical variables into dummy values.

```{r}
tree_rec <- recipe(Diabetes_binary ~ ., data = data_train) |>
  step_dummy(HighBP, HighChol, HeartDiseaseorAttack, PhysActivity)
tree_rec
```

Now we define the model, where we can use various values for the tree_depth and cost_complexity, and use a minimal node size of 20 (`min_n` = 20).

```{r}
tree_model <- decision_tree(tree_depth = tune(),
                            min_n = 20,
                            cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

We can then create a workflow object.

```{r}
tree_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_model)
```

We'll use 5 fold CV to select our tuning parameters using a tuning grid. Here we'll set the number of the values ourselves with `grid_regular()`.

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(10,5))
```

Now we can use `tune_grid()` with this specification on our `data_5_fold` object

```{r}
tree_fits <- tree_wkf |>
  tune_grid(resamples = data_5_fold,
            grid = tree_grid,
            metrics = metric_set(mn_log_loss))
tree_fits
```

We can then see which tuning parameters are best.

```{r}
tree_fits |>
  collect_metrics() |>
  filter(.metric == "mn_log_loss") |>
  arrange(mean)
```

We can then grab our best classification tree model, which has a `cost_complexity` of 1e-10 and a `tree_depth` of 11.

```{r}
best_tree <- select_best(tree_fits, metric = "mn_log_loss")
best_tree
```

We can then refit on the entire training set using these tuning parameters.

```{r}
tree_final_wkf <- tree_wkf |>
  finalize_workflow(best_tree)
tree_final_fit <- tree_final_wkf |>
  last_fit(data_split, metrics = metric_set(mn_log_loss))
```

Now we can see how it performed on the test set metrics. We'll want to compare this to the best model from the Random Forest family (which we we'll fit in the next section) to determine which is the best model out of these two families of models.

```{r}
tree_final_fit |> collect_metrics()
```

# Random Forest