[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "Introduction\nDiabetes is among the most prevalent chronic diseases in the United States, impacting millions of American each year and exerting a significant financial burden on the economy. Diabetes is a serious chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy.\nComplications like heart disease, vision loss, lower-limb amputation, and kidney disease are associated with chronically high levels of sugar remaining in the bloodstream for those with diabetes. While there is no cure, strategies like losing weight, eating healthy, being active, and receiving medical treatments can mitigate the harms of this disease in many patients. Early diagnosis can lead to lifestyle changes and more effective treatment, making predictive models for diabetes risk important tools for public and public health officials.\nThe data set used diabetes_binary_health_indicators_BRFSS2015.csv is a clean dataset of 253,680 survey responses to the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) 2015 survey. The target variable Diabetes_binary has 2 classes: 0 for no diabetes and 1 for prediabetes or diabetes. While the data set has 21 feature variables, for the purposes of this analysis, we will use five predictor variables.\n\nHighBP: Binary variable where 0 is no high blood pressure (BP) and 1 is high BP\nHighChol: Binary variable where 0 is no high cholesterol and 1 is high cholesterol\nBMI: Continuous variable for Body Mass Index\nHeartDiseaseorAttack: Binary variable for coronary heart disease (CHD) or myocardial infection (MI), where 0 is no and 1 is yes.\nPhysActivity: Binary variable where 0 is no physical activity in the past 30 days (not including job) and 1 is yes (some physical activity)\n\nIn our prior page we explored our data to gain insight into the data’s structure, variable distribution, and potential relationships between the predictor variables and the Diabetes_binary variable. We saw that those who had diabetes often had high blood pressure and high cholesterol. We also saw that there wasn’t quite the same relationship between those who had diabetes and those who had heart diseases or attacks and those who were not physically active. Although we will still keep these variables in as predictors.\nNow our goal will be to create models for predicting the Diabetes_binary variable (using tidymodels). We’ll use log-loss as our metric to evaluate the models. We’ll create a classification tree model and a random forest model, using log-loss with 5 fold cross-validation to select the best model from these two families of models.\n\n\nData\nWe can first start by reading in the data and doing the same data cleaning as in our EDA file (i.e. making the binary variables into factors). We’ll also split the data into a training (70% of the data) and a test set (30% of the data). We also can set up our our 5 fold CV split based on the training data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nfull_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(full_data)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\nfull_data &lt;- full_data |&gt;\n  mutate(\n    HighBP = factor(HighBP, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No\", \"Yes\"))\n  ) |&gt;\n  select(Diabetes_binary, HighBP, HighChol, HeartDiseaseorAttack, PhysActivity, BMI)\n\nhead(full_data)\n\n# A tibble: 6 × 6\n  Diabetes_binary HighBP HighChol HeartDiseaseorAttack PhysActivity   BMI\n  &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;                &lt;fct&gt;        &lt;dbl&gt;\n1 No              Yes    Yes      No                   No              40\n2 No              No     No       No                   Yes             25\n3 No              Yes    Yes      No                   No              28\n4 No              Yes    No       No                   Yes             27\n5 No              Yes    Yes      No                   Yes             24\n6 No              Yes    Yes      No                   Yes             25\n\nset.seed(123)\ndata_split &lt;- initial_split(full_data, prop = 0.7)\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\ndata_5_fold &lt;- vfold_cv(data_train, 5)\n\nhead(data_train)\n\n# A tibble: 6 × 6\n  Diabetes_binary HighBP HighChol HeartDiseaseorAttack PhysActivity   BMI\n  &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;                &lt;fct&gt;        &lt;dbl&gt;\n1 No              No     No       No                   Yes             23\n2 No              Yes    No       No                   Yes             35\n3 No              No     No       No                   No              20\n4 Yes             Yes    No       No                   Yes             28\n5 No              Yes    No       No                   Yes             29\n6 No              No     Yes      Yes                  Yes             23\n\nhead(data_test)\n\n# A tibble: 6 × 6\n  Diabetes_binary HighBP HighChol HeartDiseaseorAttack PhysActivity   BMI\n  &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;                &lt;fct&gt;        &lt;dbl&gt;\n1 No              No     No       No                   Yes             25\n2 No              Yes    No       No                   Yes             27\n3 No              Yes    Yes      No                   Yes             24\n4 No              Yes    Yes      No                   Yes             25\n5 Yes             Yes    Yes      No                   No              28\n6 No              No     Yes      No                   Yes             33\n\n\n\n\nClassification Tree\nFor our first model we will fit a Classification Tree model. Tree based methods are flexible and attempt to split up predictor space into regions. On each region, a different prediction can then be made and adjacent regions do not need to have predictions close to each other. For a Classification Tree model our response in a categorical variable, in our case the Diabetes_binary variable, that we try to classify (predict) group membership. We then make our prediction based on which bin an observation ends up in, with the most prevalent class in a bin being used as our prediction. We’ll fit a classification tree with varying values for the complexity parameter. We can then use log-loss as the metric based on the 5 fold CV on the training set to evaluate the performance of the different classification tree models to determine the best model in this family.\nWe can now fit our Classification trees. First we define the recipe, where we can make our categorical variables into dummy values.\n\ntree_rec &lt;- recipe(Diabetes_binary ~ ., data = data_train) |&gt;\n  step_dummy(HighBP, HighChol, HeartDiseaseorAttack, PhysActivity)\ntree_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: HighBP, HighChol, HeartDiseaseorAttack, ...\n\n\nNow we define the model, where we can use various values for the tree_depth and cost_complexity, and use a minimal node size of 20 (min_n = 20).\n\ntree_model &lt;- decision_tree(tree_depth = tune(),\n                            min_n = 20,\n                            cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nWe can then create a workflow object.\n\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_model)\n\nWe’ll use 5 fold CV to select our tuning parameters using a tuning grid. Here we’ll set the number of the values ourselves with grid_regular().\n\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10,5))\n\nNow we can use tune_grid() with this specification on our data_5_fold object\n\ntree_fits &lt;- tree_wkf |&gt;\n  tune_grid(resamples = data_5_fold,\n            grid = tree_grid,\n            metrics = metric_set(mn_log_loss))\ntree_fits\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits                 id    .metrics          .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [142060/35516]&gt; Fold1 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [142061/35515]&gt; Fold2 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [142061/35515]&gt; Fold3 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [142061/35515]&gt; Fold4 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [142061/35515]&gt; Fold5 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nWe can then see which tuning parameters are best.\n\ntree_fits |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 50 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1    0.0000000001         11 mn_log_loss binary     0.349     5 0.00129 Prepro…\n 2    0.000000001          11 mn_log_loss binary     0.349     5 0.00129 Prepro…\n 3    0.00000001           11 mn_log_loss binary     0.349     5 0.00129 Prepro…\n 4    0.0000001            11 mn_log_loss binary     0.349     5 0.00129 Prepro…\n 5    0.000001             11 mn_log_loss binary     0.349     5 0.00129 Prepro…\n 6    0.0000000001          8 mn_log_loss binary     0.349     5 0.00107 Prepro…\n 7    0.000000001           8 mn_log_loss binary     0.349     5 0.00107 Prepro…\n 8    0.00000001            8 mn_log_loss binary     0.349     5 0.00107 Prepro…\n 9    0.0000001             8 mn_log_loss binary     0.349     5 0.00107 Prepro…\n10    0.000001              8 mn_log_loss binary     0.349     5 0.00107 Prepro…\n# ℹ 40 more rows\n\n\nWe can then grab our best classification tree model, which has a cost_complexity of 1e-10 and a tree_depth of 11.\n\nbest_tree &lt;- select_best(tree_fits, metric = \"mn_log_loss\")\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         11 Preprocessor1_Model31\n\n\nWe can then refit on the entire training set using these tuning parameters.\n\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(best_tree)\ntree_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(mn_log_loss))\n\nNow we can see how it performed on the test set metrics. We’ll want to compare this to the best model from the Random Forest family (which we we’ll fit in the next section) to determine which is the best model out of these two families of models.\n\ntree_final_fit |&gt; collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.353 Preprocessor1_Model1\n\n\n\n\nRandom Forest\nNow we will move onto our second family of models, a Random Forest model. A random forest model is an ensemble method that combines the predictions of multiple decision trees to make a more robust and accurate prediction. Instead of relying on one tree, it generates many fitted trees and aggregates their predictions. For classification trees, it usually combines the predictions from all of the fitted classification trees and averages the results using the majority vote, or the most common prediction made by all of the bootstrap trees. What makes a random forest model special is that it doesn’t use all of the predictors in each step, and instead considers splits using a random subset of predictors each time, where this number mtry is a tuning parameter. By randomly selecting a subset of predictors, a good predictor or two won’t dominate the tree fits.\nWe can now define our random forest model, with the same recipe as before.\n\nrf_rec &lt;- recipe(Diabetes_binary ~ ., data = data_train) |&gt;\n  step_dummy(HighBP, HighChol, HeartDiseaseorAttack, PhysActivity)\nrf_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: HighBP, HighChol, HeartDiseaseorAttack, ...\n\n\nWe can then define the random forest model.\n\nrf_model &lt;- rand_forest(mtry = tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\nWe then combine our recipe and our model into a workflow object.\n\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_model)\n\nWe can then fit the random forest model to our 5 CV folds.\n\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(resamples = data_5_fold,\n            grid = 7,\n            metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWe can now examine our log-loss metric across the folds to determine the best mtry value.\n\nrf_fit |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"mn_log_loss\") |&gt;\n  arrange(mean)\n\n# A tibble: 5 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     3 mn_log_loss binary     0.338     5 0.000841 Preprocessor1_Model5\n2     4 mn_log_loss binary     0.339     5 0.000807 Preprocessor1_Model4\n3     2 mn_log_loss binary     0.340     5 0.000827 Preprocessor1_Model3\n4     5 mn_log_loss binary     0.345     5 0.00122  Preprocessor1_Model1\n5     1 mn_log_loss binary     0.349     5 0.000787 Preprocessor1_Model2\n\n\nWe can then grab our best tuning parameter.\n\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\nrf_best_params\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;int&gt; &lt;chr&gt;               \n1     3 Preprocessor1_Model5\n\n\nWe can then refit on the entire training set using this tuning parameter.\n\nrf_final_wkf &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params)\nrf_final_fit &lt;- rf_final_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(mn_log_loss))\n\nNow we can see how it performs on the test set metrics. In the next section we’ll directly compare the two models we fit on their test set metrics to determine the final model and overall winner!\n\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.340 Preprocessor1_Model1\n\n\n\n\nFinal Model Selection\nNow we have two best models for each model type. For our Classification Tree Model, we determined that the best parameters are a cost_complexity of 1e-10 and a tree_depth of 11, with our pre-set min_n being 20. For our Random Forest Model, the best model had a mtry of 3.\nWe can then compare these models to the test set log-loss metric.\n\ntree_final_fit |&gt; collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.353 Preprocessor1_Model1\n\n\n\nrf_final_fit |&gt; collect_metrics()\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.340 Preprocessor1_Model1\n\n\nHere we see that based on how our models perform on the test set, our best model and overall winner is the Random Forest model with mtry of 3. We’ll use this model in our API development."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Introduction\nDiabetes is among the most prevalent chronic diseases in the United States, impacting millions of American each year and exerting a significant financial burden on the economy. Diabetes is a serious chronic disease in which individuals lose the ability to effectively regulate levels of glucose in the blood, and can lead to reduced quality of life and life expectancy. It is generally characterized by either the body not making enough insulin or being unable to use the insulin that is made as effectively as needed.\nComplications like heart disease, vision loss, lower-limb amputation, and kidney disease are associated with chronically high levels of sugar remaining in the bloodstream for those with diabetes. While there is no cure, strategies like losing weight, eating healthy, being active, and receiving medical treatments can mitigate the harms of this disease in many patients. Early diagnosis can lead to lifestyle changes and more effective treatment, making predictive models for diabetes risk important tools for public and public health officials.\nThe data set used diabetes_binary_health_indicators_BRFSS2015.csv is a clean dataset of 253,680 survey responses to the CDC’s Behavioral Risk Factor Surveillance System (BRFSS) 2015 survey. The target variable Diabetes_binary has 2 classes: 0 for no diabetes and 1 for prediabetes or diabetes. While the data set has 21 feature variables, for the purposes of this analysis, we will use five predictor variables.\n\nHighBP: Binary variable where 0 is no high blood pressure (BP) and 1 is high BP\nHighChol: Binary variable where 0 is no high cholesterol and 1 is high cholesterol\nBMI: Continuous variable for Body Mass Index\nHeartDiseaseorAttack: Binary variable for coronary heart disease (CHD) or myocardial infection (MI), where 0 is no and 1 is yes.\nPhysActivity: Binary variable where 0 is no physical activity in the past 30 days (not including job) and 1 is yes (some physical activity)\n\nThrough Exploratory Data Analysis (EDA) we can gain insight into the data’s structure, variable distribution, and potential relationships between the predictor variables and the target variable of interest. After EDA, we will move into the modeling stage of the analysis, where our goal will be to create models for predicting the Diabetes_binary variable using our selected predictor variables.\n\n\nData\nNow we can use a relative path to import the data. Additionally, we will want to convert our binary variables to factors with meaningful level names and check on any missingness.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nfull_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(full_data)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\nfull_data &lt;- full_data |&gt;\n  mutate(\n    HighBP = factor(HighBP, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0,1), labels = c(\"No\", \"Yes\")),\n    Diabetes_binary = factor(Diabetes_binary, levels = c(0,1), labels = c(\"No\", \"Yes\"))\n  ) |&gt;\n  select(Diabetes_binary, HighBP, HighChol, HeartDiseaseorAttack, PhysActivity, BMI)\n\nhead(full_data)\n\n# A tibble: 6 × 6\n  Diabetes_binary HighBP HighChol HeartDiseaseorAttack PhysActivity   BMI\n  &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;                &lt;fct&gt;        &lt;dbl&gt;\n1 No              Yes    Yes      No                   No              40\n2 No              No     No       No                   Yes             25\n3 No              Yes    Yes      No                   No              28\n4 No              Yes    No       No                   Yes             27\n5 No              Yes    Yes      No                   Yes             24\n6 No              Yes    Yes      No                   Yes             25\n\nsum_na &lt;- function(column){\n  sum(is.na(column))\n}\n\nna_counts &lt;- full_data |&gt;\n  summarize(across(everything(), sum_na))\nna_counts\n\n# A tibble: 1 × 6\n  Diabetes_binary HighBP HighChol HeartDiseaseorAttack PhysActivity   BMI\n            &lt;int&gt;  &lt;int&gt;    &lt;int&gt;                &lt;int&gt;        &lt;int&gt; &lt;int&gt;\n1               0      0        0                    0            0     0\n\n\nAs we can see, now our data has meaningful level names for the binary variables, and we identified that there is no missingness in the data.\n\n\nSummarizations\nNow we can investigate our data using meaningful summary statistics and plots. It’s important to note that although it is best practice to split the data into a training and testing set first, we are told in the project instructions to do EDA on the full data.\nWe can first start with examining our binary variables through contingency tables. Here we see that our data has a lot more individuals without diabetes than those with prediabetes or diabetes.\n\ntable(full_data$Diabetes_binary)\n\n\n    No    Yes \n218334  35346 \n\n\nNow we may want to see how the counts are for our different binary predictors and our Diabetes_binary variable. Here we see that of those who have diabetes, there are more individuals who have High Blood Pressure, and those who do not have diabetes do not have High Blood Pressure.\n\ntable(full_data[,c(\"Diabetes_binary\", \"HighBP\")])\n\n               HighBP\nDiabetes_binary     No    Yes\n            No  136109  82225\n            Yes   8742  26604\n\n\nWe can see a similar trend for High Cholesterol, where there are more individuals among those who have diabetes that have High Cholesterol and more individuals among those who don’t have diabetes that don’t have High Cholesterol.\n\ntable(full_data[,c(\"Diabetes_binary\", \"HighChol\")])\n\n               HighChol\nDiabetes_binary     No    Yes\n            No  134429  83905\n            Yes  11660  23686\n\n\nFor our HeartDiseaseorAttack variable, we don’t see a similar trend as we did in High BP and High Chol. Instead, we see that for those who have and don’t have diabetes, more individuals do not have a heart disease or attack. However, this variable could still be a predictor for Diabetes.\n\ntable(full_data[,c(\"Diabetes_binary\", \"HeartDiseaseorAttack\")])\n\n               HeartDiseaseorAttack\nDiabetes_binary     No    Yes\n            No  202319  16015\n            Yes  27468   7878\n\n\nSimilarly, we see that of those who have and don’t have diabetes, there are more individuals who are physically active than those who aren’t. So physical activity may still be a predictor for Diabetes, in that by being more phyiscally active you may reduce your risk.\n\ntable(full_data[,c(\"Diabetes_binary\", \"PhysActivity\")])\n\n               PhysActivity\nDiabetes_binary     No    Yes\n            No   48701 169633\n            Yes  13059  22287\n\n\nWe could also summarize all of this into a graphical view of the categorical variable counts across the levels of Diabetes_binary.\n\nlibrary(ggplot2)\ncategorical_vars &lt;- c(\"HighBP\", \"HighChol\", \"HeartDiseaseorAttack\", \"PhysActivity\")\n\npar(mfrow = c(2,2))\nfor(var in categorical_vars) {\n  cat &lt;- ggplot(full_data, aes_string(x = var, fill = \"Diabetes_binary\")) +\n    geom_bar(position = \"dodge\") +\n    labs(title = paste(var, \"vs. Diabetes_binary\"), x = var, y = \"Count\") +\n    theme_minimal() +\n    scale_fill_discrete(\"Diabetes?\")\n  print(cat)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we can investigate our BMI distribution, as it is a continuous variable, via a histogram.\n\nggplot(full_data, aes(x = BMI)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\", alpha = 0.5) +\n  labs(title = \"Distribution of BMI\", x = \"BMI\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can also examine the summary statistics for BMI across the Diabetes_binary variable. Here we see that those who have diabetes have a higher mean and median BMI.\n\nfull_data |&gt;\n  group_by(Diabetes_binary) |&gt;\n  summarize(across(where(is.numeric),\n                   list(\"mean\" = ~mean(.x), \"median\" = ~median(.x), \"max\" = ~max(.x),\n                        \"min\" = ~min(.x), \"sd\" = ~sd(.x)),\n                   .names = \"{.fn}_{.col}\"))\n\n# A tibble: 2 × 6\n  Diabetes_binary mean_BMI median_BMI max_BMI min_BMI sd_BMI\n  &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 No                  27.8         27      98      12   6.29\n2 Yes                 31.9         31      98      13   7.36\n\n\n\n\nLink to Modeling page\nNow that we did some EDA, we can move onto the modeling portion of the analysis.\nClick here for the Modeling Page"
  }
]